{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Generated 18 settings to run.\n",
      "Running example_hparam_run_4 0 with settings:\n",
      "{'init': {'n_ts_features': 9, 'n_static_features': 1, 'hidden_size': 64, 'num_layers': 3}, 'fit': {'batch_size': 32, 'num_epochs': 50, 'device': 'cuda', 'optim_fn': <class 'torch.optim.adam.Adam'>, 'optim_kwargs': {'lr': 0.0001, 'weight_decay': 0.0001}, 'scheduler_fn': <class 'torch.optim.lr_scheduler.StepLR'>, 'scheduler_kwargs': {'step_size': 1, 'gamma': 0.99}, 'val_fraction': 0.1, 'val_split_by_year': True, 'seed': 16, 'do_early_stopping': True}}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path so we can import project files in notebook\n",
    "current_dir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "lib_path = os.path.join(current_dir, \"..\")\n",
    "sys.path.append(lib_path)\n",
    "\n",
    "from runs.validate_model import validate_single_model\n",
    "from models.nn_models import ExampleLSTM\n",
    "from util.data import generate_settings, update_settings\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Set random seed for reproducibility for random, numpy, and torch\n",
    "seed = 16\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "model_name_base = \"ExampleLSTM_test\"\n",
    "model_constructor = ExampleLSTM\n",
    "base_args = {\n",
    "    \"init\": {\n",
    "        \"n_ts_features\": 9,\n",
    "        \"n_static_features\": 1,\n",
    "        \"hidden_size\": 32,\n",
    "        \"num_layers\": 2,\n",
    "    },\n",
    "\n",
    "    \"fit\": {\n",
    "        'batch_size': 32,\n",
    "        'num_epochs': 50,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'optim_fn': torch.optim.Adam,\n",
    "        'optim_kwargs': {\"lr\": 0.0001, 'weight_decay': 0.00001},\n",
    "        'scheduler_fn': torch.optim.lr_scheduler.StepLR,\n",
    "        'scheduler_kwargs': {\"step_size\": 1, \"gamma\": 0.99},\n",
    "        'val_fraction': 0.1,\n",
    "        'val_split_by_year': True,\n",
    "        'seed': 16,\n",
    "        'do_early_stopping': True,\n",
    "    }\n",
    "}\n",
    "\n",
    "run_name_base = \"example_hparam_run_4\"\n",
    "\n",
    "# Define hyperparameters to search over and their values\n",
    "param_space = {\n",
    "    \"init.hidden_size\": [16, 32, 64],\n",
    "    \"init.num_layers\": [1, 2, 3],\n",
    "    \"fit.optim_kwargs.lr\": [0.0001, 0.001],\n",
    "    \"fit.optim_kwargs.weight_decay\": [0.0001],\n",
    "}\n",
    "\n",
    "\n",
    "settings = generate_settings(param_space, base_args)\n",
    "print(f\"Generated {len(settings)} settings to run.\")\n",
    "\n",
    "# Create base folder for run_name\n",
    "Path(f\"results/{run_name_base}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Shuffle settings\n",
    "random.shuffle(settings)\n",
    "\n",
    "# Run the hyperparameter search   \n",
    "for i, setting in enumerate(settings):\n",
    "\n",
    "    run_kwargs = update_settings(setting, base_args)\n",
    "\n",
    "    print(f\"Running {run_name_base} {i} with settings:\")\n",
    "    print(run_kwargs)\n",
    "\n",
    "    result_df = validate_single_model(\n",
    "        run_name_base + '_' + str(i),\n",
    "        model_name_base + '_' + str(i),\n",
    "        model_constructor,\n",
    "        run_kwargs['init'],\n",
    "        run_kwargs['fit'],\n",
    "    )\n",
    "\n",
    "    # Save the results\n",
    "    result_df.to_csv(f\"../output/runs/{run_name_base}/{run_name_base}_{i}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting 0: 2.597805378528742\n",
      "Setting 1: 3.0681170397079907\n",
      "Setting 2: 2.326060502574994\n",
      "Setting 3: 7.712058404317269\n",
      "Setting 4: 30.238999385100144\n",
      "Setting 5: 5.7340222505422735\n",
      "Setting 6: 2.3480412593254676\n",
      "Setting 7: 64.13078469496507\n",
      "Setting 8: 30.238911078526424\n",
      "Setting 9: 2.418242845397729\n",
      "Setting 10: 25.09572538962731\n",
      "Setting 11: 2.2192195630990543\n",
      "Setting 12: 5.734135274703686\n",
      "Setting 13: 7.7116414950444145\n",
      "Setting 14: 2.4967379237596807\n",
      "Setting 15: 2.1950585532646913\n",
      "Setting 16: 2.583382872434763\n",
      "Setting 17: 2.2109741820738864\n",
      "Setting 18: 2.465893642260478\n",
      "Setting 19: 60.72057254497822\n",
      "Setting 20: 30.238027425912712\n",
      "Setting 21: 2.1241133407904553\n",
      "Setting 22: 25.095947650762703\n",
      "Setting 23: 7.712051939505797\n",
      "Setting 24: 6.2202374407878285\n",
      "Setting 25: 2.3356983283391366\n",
      "Best result: 2.1241133407904553 at index 21\n",
      "Best settings: {'init': {'n_ts_features': 9, 'n_static_features': 1, 'hidden_size': 16, 'num_layers': 3}, 'fit': {'batch_size': 32, 'num_epochs': 50, 'device': 'cuda', 'optim_fn': <class 'torch.optim.adam.Adam'>, 'optim_kwargs': {'lr': 0.001, 'weight_decay': 1e-05}, 'scheduler_fn': <class 'torch.optim.lr_scheduler.StepLR'>, 'scheduler_kwargs': {'step_size': 1, 'gamma': 0.95}, 'val_fraction': 0.1, 'val_split_by_year': True, 'seed': 16}}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from runs.run_benchmark import run_benchmark\n",
    "\n",
    "\n",
    "# Loop over results and print the best one\n",
    "best_result = None\n",
    "best_result_idx = None\n",
    "for i, setting in enumerate(settings):\n",
    "    # Check if the result file exists\n",
    "    if Path(f\"results/{run_name_base}/{run_name_base}_{i}.csv\").exists():\n",
    "        result_df = pd.read_csv(f\"results/{run_name_base}/{run_name_base}_{i}.csv\")\n",
    "\n",
    "        if setting['fit']['do_early_stopping']:\n",
    "            val_loss = result_df.iloc[6, 1]\n",
    "        else:\n",
    "            val_loss = result_df.iloc[3, 1]\n",
    "        print(f\"Setting {i}: {val_loss}\")\n",
    "        if best_result is None or val_loss < best_result:\n",
    "            best_result = val_loss\n",
    "\n",
    "            best_result_idx = i\n",
    "\n",
    "print(f\"Best result: {best_result} at index {best_result_idx}\")\n",
    "print(f\"Best settings: {settings[best_result_idx]}\")\n",
    "\n",
    "# Save best results to file\n",
    "with open(f\"results/{run_name_base}/best_settings_result.txt\", \"w\") as f:\n",
    "    f.write(f\"Best result: {best_result} at index {best_result_idx}\\n\")\n",
    "\n",
    "# Save best settings dict to file with pickle\n",
    "import pickle\n",
    "settings_to_save = settings[best_result_idx]\n",
    "with open(f\"results/{run_name_base}/best_settings_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(settings_to_save, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running setting 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 13.7711: 100%|██████████| 420/420 [00:12<00:00, 33.69it/s]\n",
      "Validation Epoch 1/10 | Loss: 5.2663: 100%|██████████| 53/53 [00:01<00:00, 38.23it/s]\n",
      "Epoch 2/10 | Loss: 5.8160: 100%|██████████| 420/420 [00:11<00:00, 35.12it/s]\n",
      "Validation Epoch 2/10 | Loss: 5.6659: 100%|██████████| 53/53 [00:01<00:00, 38.82it/s]\n",
      "Epoch 3/10 | Loss: 4.5793: 100%|██████████| 420/420 [00:12<00:00, 34.81it/s]\n",
      "Validation Epoch 3/10 | Loss: 3.9131: 100%|██████████| 53/53 [00:01<00:00, 39.70it/s]\n",
      "Epoch 4/10 | Loss: 3.0510: 100%|██████████| 420/420 [00:12<00:00, 34.94it/s]\n",
      "Validation Epoch 4/10 | Loss: 3.2511: 100%|██████████| 53/53 [00:01<00:00, 36.78it/s]\n",
      "Epoch 5/10 | Loss: 2.5474: 100%|██████████| 420/420 [00:12<00:00, 33.56it/s]\n",
      "Validation Epoch 5/10 | Loss: 3.3667: 100%|██████████| 53/53 [00:01<00:00, 38.82it/s]\n",
      "Epoch 6/10 | Loss: 2.2705: 100%|██████████| 420/420 [00:12<00:00, 34.15it/s]\n",
      "Validation Epoch 6/10 | Loss: 2.3436: 100%|██████████| 53/53 [00:01<00:00, 38.07it/s]\n",
      "Epoch 7/10 | Loss: 2.0239: 100%|██████████| 420/420 [00:11<00:00, 35.13it/s]\n",
      "Validation Epoch 7/10 | Loss: 2.2451: 100%|██████████| 53/53 [00:01<00:00, 37.83it/s]\n",
      "Epoch 8/10 | Loss: 1.8921: 100%|██████████| 420/420 [00:12<00:00, 34.81it/s]\n",
      "Validation Epoch 8/10 | Loss: 2.3092: 100%|██████████| 53/53 [00:01<00:00, 37.21it/s]\n",
      "Epoch 9/10 | Loss: 1.7979: 100%|██████████| 420/420 [00:12<00:00, 34.13it/s]\n",
      "Validation Epoch 9/10 | Loss: 2.2604: 100%|██████████| 53/53 [00:01<00:00, 37.83it/s]\n",
      "Epoch 10/10 | Loss: 1.7516: 100%|██████████| 420/420 [00:12<00:00, 34.53it/s]\n",
      "Validation Epoch 10/10 | Loss: 2.3258: 100%|██████████| 53/53 [00:01<00:00, 37.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For setting 1/2, average final validation loss: 2.325794701306325\n",
      "Settings: {'batch_size': 32, 'num_epochs': 10, 'device': 'cuda', 'optim_fn': <class 'torch.optim.adam.Adam'>, 'scheduler_fn': <class 'torch.optim.lr_scheduler.StepLR'>, 'scheduler_kwargs': {'step_size': 2, 'gamma': 0.8}, 'val_fraction': 0.1, 'val_split_by_year': True, 'optim_kwargs': {'lr': 0.001, 'weight_decay': 0.0001}}\n",
      "Running setting 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 56.5167: 100%|██████████| 423/423 [00:12<00:00, 33.02it/s]\n",
      "Validation Epoch 1/10 | Loss: 25.0974: 100%|██████████| 50/50 [00:01<00:00, 39.01it/s]\n",
      "Epoch 2/10 | Loss: 11.9122: 100%|██████████| 423/423 [00:12<00:00, 34.68it/s]\n",
      "Validation Epoch 2/10 | Loss: 9.6758: 100%|██████████| 50/50 [00:01<00:00, 38.86it/s] \n",
      "Epoch 3/10 | Loss: 6.9333: 100%|██████████| 423/423 [00:12<00:00, 34.27it/s]\n",
      "Validation Epoch 3/10 | Loss: 6.3329: 100%|██████████| 50/50 [00:01<00:00, 38.04it/s]\n",
      "Epoch 4/10 | Loss: 6.1320: 100%|██████████| 423/423 [00:12<00:00, 34.79it/s]\n",
      "Validation Epoch 4/10 | Loss: 5.2698: 100%|██████████| 50/50 [00:01<00:00, 35.23it/s]\n",
      "Epoch 5/10 | Loss: 6.0255: 100%|██████████| 423/423 [00:12<00:00, 33.72it/s]\n",
      "Validation Epoch 5/10 | Loss: 5.0155: 100%|██████████| 50/50 [00:01<00:00, 40.60it/s]\n",
      "Epoch 6/10 | Loss: 6.0127: 100%|██████████| 423/423 [00:12<00:00, 35.01it/s]\n",
      "Validation Epoch 6/10 | Loss: 4.9434: 100%|██████████| 50/50 [00:01<00:00, 38.88it/s]\n",
      "Epoch 7/10 | Loss: 6.0015: 100%|██████████| 423/423 [00:12<00:00, 34.62it/s]\n",
      "Validation Epoch 7/10 | Loss: 4.8959: 100%|██████████| 50/50 [00:01<00:00, 37.26it/s]\n",
      "Epoch 8/10 | Loss: 5.9862: 100%|██████████| 423/423 [00:12<00:00, 34.59it/s]\n",
      "Validation Epoch 8/10 | Loss: 4.9402: 100%|██████████| 50/50 [00:01<00:00, 37.11it/s]\n",
      "Epoch 9/10 | Loss: 5.9358: 100%|██████████| 423/423 [00:12<00:00, 34.35it/s]\n",
      "Validation Epoch 9/10 | Loss: 4.9252: 100%|██████████| 50/50 [00:01<00:00, 39.17it/s]\n",
      "Epoch 10/10 | Loss: 5.8171: 100%|██████████| 423/423 [00:11<00:00, 35.72it/s]\n",
      "Validation Epoch 10/10 | Loss: 4.7878: 100%|██████████| 50/50 [00:01<00:00, 39.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For setting 2/2, average final validation loss: 4.787838568687439\n",
      "Settings: {'batch_size': 32, 'num_epochs': 10, 'device': 'cuda', 'optim_fn': <class 'torch.optim.adam.Adam'>, 'scheduler_fn': <class 'torch.optim.lr_scheduler.StepLR'>, 'scheduler_kwargs': {'step_size': 2, 'gamma': 0.8}, 'val_fraction': 0.1, 'val_split_by_year': True, 'optim_kwargs': {'lr': 0.0001, 'weight_decay': 0.0001}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 13.6868: 100%|██████████| 417/417 [00:12<00:00, 33.11it/s]\n",
      "Validation Epoch 1/10 | Loss: 4.1956: 100%|██████████| 56/56 [00:01<00:00, 38.55it/s]\n",
      "Epoch 2/10 | Loss: 5.9236: 100%|██████████| 417/417 [00:12<00:00, 34.33it/s]\n",
      "Validation Epoch 2/10 | Loss: 3.7410: 100%|██████████| 56/56 [00:01<00:00, 39.86it/s]\n",
      "Epoch 3/10 | Loss: 5.4929: 100%|██████████| 417/417 [00:11<00:00, 35.10it/s]\n",
      "Validation Epoch 3/10 | Loss: 3.5991: 100%|██████████| 56/56 [00:01<00:00, 38.49it/s]\n",
      "Epoch 4/10 | Loss: 5.1651: 100%|██████████| 417/417 [00:12<00:00, 34.42it/s]\n",
      "Validation Epoch 4/10 | Loss: 3.4802: 100%|██████████| 56/56 [00:01<00:00, 37.52it/s]\n",
      "Epoch 5/10 | Loss: 4.4667: 100%|██████████| 417/417 [00:12<00:00, 34.32it/s]\n",
      "Validation Epoch 5/10 | Loss: 2.8402: 100%|██████████| 56/56 [00:01<00:00, 36.54it/s]\n",
      "Epoch 6/10 | Loss: 2.7687: 100%|██████████| 417/417 [00:12<00:00, 33.87it/s]\n",
      "Validation Epoch 6/10 | Loss: 2.6725: 100%|██████████| 56/56 [00:01<00:00, 38.12it/s]\n",
      "Epoch 7/10 | Loss: 2.2086: 100%|██████████| 417/417 [00:11<00:00, 35.11it/s]\n",
      "Validation Epoch 7/10 | Loss: 2.5860: 100%|██████████| 56/56 [00:01<00:00, 40.18it/s]\n",
      "Epoch 8/10 | Loss: 1.9997: 100%|██████████| 417/417 [00:11<00:00, 35.83it/s]\n",
      "Validation Epoch 8/10 | Loss: 2.5203: 100%|██████████| 56/56 [00:01<00:00, 40.25it/s]\n",
      "Epoch 9/10 | Loss: 1.8552: 100%|██████████| 417/417 [00:11<00:00, 35.96it/s]\n",
      "Validation Epoch 9/10 | Loss: 2.2862: 100%|██████████| 56/56 [00:01<00:00, 40.96it/s]\n",
      "Epoch 10/10 | Loss: 1.7762: 100%|██████████| 417/417 [00:11<00:00, 36.17it/s]\n",
      "Validation Epoch 10/10 | Loss: 2.3707: 100%|██████████| 56/56 [00:01<00:00, 39.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final validation loss of outer fold: 2.370673019438982\n",
      "Final best setting of outer fold: {'batch_size': 32, 'num_epochs': 10, 'device': 'cuda', 'optim_fn': <class 'torch.optim.adam.Adam'>, 'scheduler_fn': <class 'torch.optim.lr_scheduler.StepLR'>, 'scheduler_kwargs': {'step_size': 2, 'gamma': 0.8}, 'val_fraction': 0.1, 'val_split_by_year': True, 'optim_kwargs': {'lr': 0.001, 'weight_decay': 0.0001}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Loss: 37.9866: 100%|██████████| 421/421 [00:12<00:00, 32.88it/s]\n",
      "Validation Epoch 1/50 | Loss: 10.8131: 100%|██████████| 52/52 [00:01<00:00, 40.99it/s]\n",
      "Epoch 2/50 | Loss: 7.3026: 100%|██████████| 421/421 [00:12<00:00, 34.02it/s]\n",
      "Validation Epoch 2/50 | Loss: 5.1150: 100%|██████████| 52/52 [00:01<00:00, 38.69it/s]\n",
      "Epoch 3/50 | Loss: 6.0177: 100%|██████████| 421/421 [00:12<00:00, 34.34it/s]\n",
      "Validation Epoch 3/50 | Loss: 4.8725: 100%|██████████| 52/52 [00:01<00:00, 39.41it/s]\n",
      "Epoch 4/50 | Loss: 6.0037: 100%|██████████| 421/421 [00:12<00:00, 34.28it/s]\n",
      "Validation Epoch 4/50 | Loss: 4.8704: 100%|██████████| 52/52 [00:01<00:00, 39.48it/s]\n",
      "Epoch 5/50 | Loss: 5.9919: 100%|██████████| 421/421 [00:12<00:00, 33.87it/s]\n",
      "Validation Epoch 5/50 | Loss: 4.8879: 100%|██████████| 52/52 [00:01<00:00, 40.45it/s]\n",
      "Epoch 6/50 | Loss: 5.9244: 100%|██████████| 421/421 [00:12<00:00, 34.47it/s]\n",
      "Validation Epoch 6/50 | Loss: 4.7264: 100%|██████████| 52/52 [00:01<00:00, 37.99it/s]\n",
      "Epoch 7/50 | Loss: 4.0916: 100%|██████████| 421/421 [00:12<00:00, 32.44it/s]\n",
      "Validation Epoch 7/50 | Loss: 2.7054: 100%|██████████| 52/52 [00:01<00:00, 37.68it/s]\n",
      "Epoch 8/50 | Loss: 2.8922: 100%|██████████| 421/421 [00:12<00:00, 33.73it/s]\n",
      "Validation Epoch 8/50 | Loss: 2.8393: 100%|██████████| 52/52 [00:01<00:00, 36.29it/s]\n",
      "Epoch 9/50 | Loss: 2.4967: 100%|██████████| 421/421 [00:12<00:00, 32.86it/s]\n",
      "Validation Epoch 9/50 | Loss: 2.4273: 100%|██████████| 52/52 [00:01<00:00, 38.13it/s]\n",
      "Epoch 10/50 | Loss: 2.2161: 100%|██████████| 421/421 [00:12<00:00, 34.31it/s]\n",
      "Validation Epoch 10/50 | Loss: 2.5836: 100%|██████████| 52/52 [00:01<00:00, 40.09it/s]\n",
      "Epoch 11/50 | Loss: 2.0539: 100%|██████████| 421/421 [00:12<00:00, 33.74it/s]\n",
      "Validation Epoch 11/50 | Loss: 2.4010: 100%|██████████| 52/52 [00:01<00:00, 39.13it/s]\n",
      "Epoch 12/50 | Loss: 1.9495: 100%|██████████| 421/421 [00:12<00:00, 32.86it/s]\n",
      "Validation Epoch 12/50 | Loss: 2.3397: 100%|██████████| 52/52 [00:01<00:00, 37.82it/s]\n",
      "Epoch 13/50 | Loss: 1.8752: 100%|██████████| 421/421 [00:12<00:00, 32.92it/s]\n",
      "Validation Epoch 13/50 | Loss: 2.3343: 100%|██████████| 52/52 [00:01<00:00, 38.31it/s]\n",
      "Epoch 14/50 | Loss: 1.8082: 100%|██████████| 421/421 [00:12<00:00, 33.45it/s]\n",
      "Validation Epoch 14/50 | Loss: 2.4745: 100%|██████████| 52/52 [00:01<00:00, 37.52it/s]\n",
      "Epoch 15/50 | Loss: 1.7411: 100%|██████████| 421/421 [00:12<00:00, 33.71it/s]\n",
      "Validation Epoch 15/50 | Loss: 2.6444: 100%|██████████| 52/52 [00:01<00:00, 37.38it/s]\n",
      "Epoch 16/50 | Loss: 1.7044: 100%|██████████| 421/421 [00:12<00:00, 34.28it/s]\n",
      "Validation Epoch 16/50 | Loss: 2.7351: 100%|██████████| 52/52 [00:01<00:00, 40.06it/s]\n",
      "Epoch 17/50 | Loss: 1.6656: 100%|██████████| 421/421 [00:12<00:00, 34.38it/s]\n",
      "Validation Epoch 17/50 | Loss: 2.4390: 100%|██████████| 52/52 [00:01<00:00, 39.03it/s]\n",
      "Epoch 18/50 | Loss: 1.6409: 100%|██████████| 421/421 [00:12<00:00, 33.60it/s]\n",
      "Validation Epoch 18/50 | Loss: 2.4124: 100%|██████████| 52/52 [00:01<00:00, 38.56it/s]\n",
      "Epoch 19/50 | Loss: 1.6055: 100%|██████████| 421/421 [00:12<00:00, 33.82it/s]\n",
      "Validation Epoch 19/50 | Loss: 2.8550: 100%|██████████| 52/52 [00:01<00:00, 38.06it/s]\n",
      "Epoch 20/50 | Loss: 1.5806: 100%|██████████| 421/421 [00:12<00:00, 32.85it/s]\n",
      "Validation Epoch 20/50 | Loss: 2.6060: 100%|██████████| 52/52 [00:01<00:00, 37.23it/s]\n",
      "Epoch 21/50 | Loss: 1.5624: 100%|██████████| 421/421 [00:12<00:00, 32.63it/s]\n",
      "Validation Epoch 21/50 | Loss: 2.7153: 100%|██████████| 52/52 [00:01<00:00, 37.26it/s]\n",
      "Epoch 22/50 | Loss: 1.5345: 100%|██████████| 421/421 [00:12<00:00, 32.83it/s]\n",
      "Validation Epoch 22/50 | Loss: 2.9160: 100%|██████████| 52/52 [00:01<00:00, 39.19it/s]\n",
      "Epoch 23/50 | Loss: 1.5382: 100%|██████████| 421/421 [00:12<00:00, 33.29it/s]\n",
      "Validation Epoch 23/50 | Loss: 2.7675: 100%|██████████| 52/52 [00:01<00:00, 36.32it/s]\n",
      "Epoch 24/50 | Loss: 1.5160: 100%|██████████| 421/421 [00:12<00:00, 33.27it/s]\n",
      "Validation Epoch 24/50 | Loss: 2.7352: 100%|██████████| 52/52 [00:01<00:00, 38.57it/s]\n",
      "Epoch 25/50 | Loss: 1.5003: 100%|██████████| 421/421 [00:12<00:00, 33.55it/s]\n",
      "Validation Epoch 25/50 | Loss: 2.7039: 100%|██████████| 52/52 [00:01<00:00, 37.91it/s]\n",
      "Epoch 26/50 | Loss: 1.4903: 100%|██████████| 421/421 [00:12<00:00, 33.35it/s]\n",
      "Validation Epoch 26/50 | Loss: 2.7137: 100%|██████████| 52/52 [00:01<00:00, 39.22it/s]\n",
      "Epoch 27/50 | Loss: 1.4804: 100%|██████████| 421/421 [00:12<00:00, 32.82it/s]\n",
      "Validation Epoch 27/50 | Loss: 2.7647: 100%|██████████| 52/52 [00:01<00:00, 38.78it/s]\n",
      "Epoch 28/50 | Loss: 1.4623: 100%|██████████| 421/421 [00:12<00:00, 32.90it/s]\n",
      "Validation Epoch 28/50 | Loss: 2.8279: 100%|██████████| 52/52 [00:01<00:00, 37.95it/s]\n",
      "Epoch 29/50 | Loss: 1.4527: 100%|██████████| 421/421 [00:12<00:00, 33.68it/s]\n",
      "Validation Epoch 29/50 | Loss: 2.8872: 100%|██████████| 52/52 [00:01<00:00, 39.54it/s]\n",
      "Epoch 30/50 | Loss: 1.4539: 100%|██████████| 421/421 [00:12<00:00, 33.46it/s]\n",
      "Validation Epoch 30/50 | Loss: 2.7710: 100%|██████████| 52/52 [00:01<00:00, 37.89it/s]\n",
      "Epoch 31/50 | Loss: 1.4329: 100%|██████████| 421/421 [00:12<00:00, 33.94it/s]\n",
      "Validation Epoch 31/50 | Loss: 2.7757: 100%|██████████| 52/52 [00:01<00:00, 38.75it/s]\n",
      "Epoch 32/50 | Loss: 1.4262: 100%|██████████| 421/421 [00:12<00:00, 34.01it/s]\n",
      "Validation Epoch 32/50 | Loss: 2.9861: 100%|██████████| 52/52 [00:01<00:00, 37.72it/s]\n",
      "Epoch 33/50 | Loss: 1.4221: 100%|██████████| 421/421 [00:12<00:00, 33.47it/s]\n",
      "Validation Epoch 33/50 | Loss: 2.9122: 100%|██████████| 52/52 [00:01<00:00, 38.69it/s]\n",
      "Epoch 34/50 | Loss: 1.4139: 100%|██████████| 421/421 [00:12<00:00, 33.00it/s]\n",
      "Validation Epoch 34/50 | Loss: 2.9871: 100%|██████████| 52/52 [00:01<00:00, 39.34it/s]\n",
      "Epoch 35/50 | Loss: 1.4120: 100%|██████████| 421/421 [00:12<00:00, 33.33it/s]\n",
      "Validation Epoch 35/50 | Loss: 2.9837: 100%|██████████| 52/52 [00:01<00:00, 38.08it/s]\n",
      "Epoch 36/50 | Loss: 1.3978: 100%|██████████| 421/421 [00:12<00:00, 33.70it/s]\n",
      "Validation Epoch 36/50 | Loss: 3.0334: 100%|██████████| 52/52 [00:01<00:00, 38.77it/s]\n",
      "Epoch 37/50 | Loss: 1.3875: 100%|██████████| 421/421 [00:12<00:00, 33.89it/s]\n",
      "Validation Epoch 37/50 | Loss: 3.0204: 100%|██████████| 52/52 [00:01<00:00, 38.14it/s]\n",
      "Epoch 38/50 | Loss: 1.3875: 100%|██████████| 421/421 [00:12<00:00, 32.39it/s]\n",
      "Validation Epoch 38/50 | Loss: 2.9799: 100%|██████████| 52/52 [00:01<00:00, 39.02it/s]\n",
      "Epoch 39/50 | Loss: 1.3754: 100%|██████████| 421/421 [00:12<00:00, 33.13it/s]\n",
      "Validation Epoch 39/50 | Loss: 2.9930: 100%|██████████| 52/52 [00:01<00:00, 37.56it/s]\n",
      "Epoch 40/50 | Loss: 1.3738: 100%|██████████| 421/421 [00:12<00:00, 33.27it/s]\n",
      "Validation Epoch 40/50 | Loss: 3.0021: 100%|██████████| 52/52 [00:01<00:00, 36.86it/s]\n",
      "Epoch 41/50 | Loss: 1.3670: 100%|██████████| 421/421 [00:12<00:00, 33.55it/s]\n",
      "Validation Epoch 41/50 | Loss: 3.0684: 100%|██████████| 52/52 [00:01<00:00, 38.97it/s]\n",
      "Epoch 42/50 | Loss: 1.3624: 100%|██████████| 421/421 [00:12<00:00, 33.59it/s]\n",
      "Validation Epoch 42/50 | Loss: 3.0900: 100%|██████████| 52/52 [00:01<00:00, 37.98it/s]\n",
      "Epoch 43/50 | Loss: 1.3581: 100%|██████████| 421/421 [00:12<00:00, 33.60it/s]\n",
      "Validation Epoch 43/50 | Loss: 3.0081: 100%|██████████| 52/52 [00:01<00:00, 38.46it/s]\n",
      "Epoch 44/50 | Loss: 1.3527: 100%|██████████| 421/421 [00:12<00:00, 33.51it/s]\n",
      "Validation Epoch 44/50 | Loss: 2.9613: 100%|██████████| 52/52 [00:01<00:00, 37.60it/s]\n",
      "Epoch 45/50 | Loss: 1.3485: 100%|██████████| 421/421 [00:12<00:00, 32.47it/s]\n",
      "Validation Epoch 45/50 | Loss: 3.0961: 100%|██████████| 52/52 [00:01<00:00, 36.91it/s]\n",
      "Epoch 46/50 | Loss: 1.3518: 100%|██████████| 421/421 [00:12<00:00, 33.19it/s]\n",
      "Validation Epoch 46/50 | Loss: 2.9793: 100%|██████████| 52/52 [00:01<00:00, 38.91it/s]\n",
      "Epoch 47/50 | Loss: 1.3405: 100%|██████████| 421/421 [00:12<00:00, 32.93it/s]\n",
      "Validation Epoch 47/50 | Loss: 3.0905: 100%|██████████| 52/52 [00:01<00:00, 38.04it/s]\n",
      "Epoch 48/50 | Loss: 1.3415: 100%|██████████| 421/421 [00:12<00:00, 33.05it/s]\n",
      "Validation Epoch 48/50 | Loss: 2.9343: 100%|██████████| 52/52 [00:01<00:00, 40.19it/s]\n",
      "Epoch 49/50 | Loss: 1.3344: 100%|██████████| 421/421 [00:12<00:00, 33.78it/s]\n",
      "Validation Epoch 49/50 | Loss: 2.9879: 100%|██████████| 52/52 [00:01<00:00, 37.80it/s]\n",
      "Epoch 50/50 | Loss: 1.3339: 100%|██████████| 421/421 [00:12<00:00, 32.76it/s]\n",
      "Validation Epoch 50/50 | Loss: 3.0585: 100%|██████████| 52/52 [00:01<00:00, 37.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running setting 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 14.0061: 100%|██████████| 417/417 [00:12<00:00, 34.08it/s]\n",
      "Validation Epoch 1/10 | Loss: 5.3538: 100%|██████████| 57/57 [00:01<00:00, 40.17it/s]\n",
      "Epoch 2/10 | Loss: 5.8437: 100%|██████████| 417/417 [00:11<00:00, 35.64it/s]\n",
      "Validation Epoch 2/10 | Loss: 4.2348: 100%|██████████| 57/57 [00:01<00:00, 39.31it/s]\n",
      "Epoch 3/10 | Loss: 5.2373: 100%|██████████| 417/417 [00:11<00:00, 35.93it/s]\n",
      "Validation Epoch 3/10 | Loss: 3.3433: 100%|██████████| 57/57 [00:01<00:00, 39.16it/s]\n",
      "Epoch 4/10 | Loss: 3.0949: 100%|██████████| 417/417 [00:11<00:00, 35.41it/s]\n",
      "Validation Epoch 4/10 | Loss: 2.4103: 100%|██████████| 57/57 [00:01<00:00, 40.80it/s]\n",
      "Epoch 5/10 | Loss: 2.3643: 100%|██████████| 417/417 [00:11<00:00, 35.56it/s]\n",
      "Validation Epoch 5/10 | Loss: 2.6295: 100%|██████████| 57/57 [00:01<00:00, 38.94it/s]\n",
      "Epoch 6/10 | Loss: 2.1288: 100%|██████████| 417/417 [00:11<00:00, 35.50it/s]\n",
      "Validation Epoch 6/10 | Loss: 2.3986: 100%|██████████| 57/57 [00:01<00:00, 39.75it/s]\n",
      "Epoch 7/10 | Loss: 1.9709: 100%|██████████| 417/417 [00:12<00:00, 34.72it/s]\n",
      "Validation Epoch 7/10 | Loss: 2.5397: 100%|██████████| 57/57 [00:01<00:00, 37.60it/s]\n",
      "Epoch 8/10 | Loss: 1.9001: 100%|██████████| 417/417 [00:11<00:00, 35.11it/s]\n",
      "Validation Epoch 8/10 | Loss: 2.2666: 100%|██████████| 57/57 [00:01<00:00, 38.85it/s]\n",
      "Epoch 9/10 | Loss: 1.8177: 100%|██████████| 417/417 [00:12<00:00, 34.06it/s]\n",
      "Validation Epoch 9/10 | Loss: 2.0851: 100%|██████████| 57/57 [00:01<00:00, 37.52it/s]\n",
      "Epoch 10/10 | Loss: 1.7795: 100%|██████████| 417/417 [00:11<00:00, 34.80it/s]\n",
      "Validation Epoch 10/10 | Loss: 2.0105: 100%|██████████| 57/57 [00:01<00:00, 37.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For setting 1/2, average final validation loss: 2.010524509245889\n",
      "Settings: {'batch_size': 32, 'num_epochs': 10, 'device': 'cuda', 'optim_fn': <class 'torch.optim.adam.Adam'>, 'scheduler_fn': <class 'torch.optim.lr_scheduler.StepLR'>, 'scheduler_kwargs': {'step_size': 2, 'gamma': 0.8}, 'val_fraction': 0.1, 'val_split_by_year': True, 'optim_kwargs': {'lr': 0.001, 'weight_decay': 0.0001}}\n",
      "Running setting 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 55.7569: 100%|██████████| 420/420 [00:12<00:00, 33.90it/s]\n",
      "Validation Epoch 1/10 | Loss: 20.7522: 100%|██████████| 53/53 [00:01<00:00, 37.19it/s]\n",
      "Epoch 2/10 | Loss: 11.5191: 100%|██████████| 420/420 [00:11<00:00, 35.89it/s]\n",
      "Validation Epoch 2/10 | Loss: 7.9371: 100%|██████████| 53/53 [00:01<00:00, 38.72it/s] \n",
      "Epoch 3/10 | Loss: 6.6626: 100%|██████████| 420/420 [00:11<00:00, 35.75it/s]\n",
      "Validation Epoch 3/10 | Loss: 6.0538: 100%|██████████| 53/53 [00:01<00:00, 40.12it/s]\n",
      "Epoch 4/10 | Loss: 6.0488: 100%|██████████| 420/420 [00:11<00:00, 35.03it/s]\n",
      "Validation Epoch 4/10 | Loss: 5.6427: 100%|██████████| 53/53 [00:01<00:00, 38.11it/s]\n",
      "Epoch 5/10 | Loss: 5.9734: 100%|██████████| 420/420 [00:12<00:00, 34.46it/s]\n",
      "Validation Epoch 5/10 | Loss: 5.5625: 100%|██████████| 53/53 [00:01<00:00, 38.56it/s]\n",
      "Epoch 6/10 | Loss: 5.9563: 100%|██████████| 420/420 [00:11<00:00, 35.03it/s]\n",
      "Validation Epoch 6/10 | Loss: 5.5391: 100%|██████████| 53/53 [00:01<00:00, 38.27it/s]\n",
      "Epoch 7/10 | Loss: 5.9026: 100%|██████████| 420/420 [00:11<00:00, 35.01it/s]\n",
      "Validation Epoch 7/10 | Loss: 5.6155: 100%|██████████| 53/53 [00:01<00:00, 39.77it/s]\n",
      "Epoch 8/10 | Loss: 5.6295: 100%|██████████| 420/420 [00:11<00:00, 35.64it/s]\n",
      "Validation Epoch 8/10 | Loss: 5.6157: 100%|██████████| 53/53 [00:01<00:00, 39.99it/s]\n",
      "Epoch 9/10 | Loss: 5.3083: 100%|██████████| 420/420 [00:11<00:00, 35.61it/s]\n",
      "Validation Epoch 9/10 | Loss: 5.4655: 100%|██████████| 53/53 [00:01<00:00, 39.51it/s]\n",
      "Epoch 10/10 | Loss: 4.8654: 100%|██████████| 420/420 [00:11<00:00, 35.55it/s]\n",
      "Validation Epoch 10/10 | Loss: 5.4002: 100%|██████████| 53/53 [00:01<00:00, 39.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For setting 2/2, average final validation loss: 5.400241127553976\n",
      "Settings: {'batch_size': 32, 'num_epochs': 10, 'device': 'cuda', 'optim_fn': <class 'torch.optim.adam.Adam'>, 'scheduler_fn': <class 'torch.optim.lr_scheduler.StepLR'>, 'scheduler_kwargs': {'step_size': 2, 'gamma': 0.8}, 'val_fraction': 0.1, 'val_split_by_year': True, 'optim_kwargs': {'lr': 0.0001, 'weight_decay': 0.0001}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 13.9753: 100%|██████████| 423/423 [00:12<00:00, 34.11it/s]\n",
      "Validation Epoch 1/10 | Loss: 4.9519: 100%|██████████| 50/50 [00:01<00:00, 39.84it/s]\n",
      "Epoch 2/10 | Loss: 5.8976: 100%|██████████| 423/423 [00:11<00:00, 35.83it/s]\n",
      "Validation Epoch 2/10 | Loss: 4.4041: 100%|██████████| 50/50 [00:01<00:00, 39.88it/s]\n",
      "Epoch 3/10 | Loss: 5.2698: 100%|██████████| 423/423 [00:11<00:00, 35.56it/s]\n",
      "Validation Epoch 3/10 | Loss: 3.5727: 100%|██████████| 50/50 [00:01<00:00, 39.20it/s]\n",
      "Epoch 4/10 | Loss: 4.5163: 100%|██████████| 423/423 [00:12<00:00, 35.00it/s]\n",
      "Validation Epoch 4/10 | Loss: 3.4700: 100%|██████████| 50/50 [00:01<00:00, 38.78it/s]\n",
      "Epoch 5/10 | Loss: 3.2917: 100%|██████████| 423/423 [00:12<00:00, 35.12it/s]\n",
      "Validation Epoch 5/10 | Loss: 3.9926: 100%|██████████| 50/50 [00:01<00:00, 37.57it/s]\n",
      "Epoch 6/10 | Loss: 2.7162: 100%|██████████| 423/423 [00:12<00:00, 34.68it/s]\n",
      "Validation Epoch 6/10 | Loss: 2.9337: 100%|██████████| 50/50 [00:01<00:00, 37.50it/s]\n",
      "Epoch 7/10 | Loss: 2.1955: 100%|██████████| 423/423 [00:11<00:00, 35.30it/s]\n",
      "Validation Epoch 7/10 | Loss: 2.6077: 100%|██████████| 50/50 [00:01<00:00, 39.26it/s]\n",
      "Epoch 8/10 | Loss: 1.9915: 100%|██████████| 423/423 [00:12<00:00, 34.56it/s]\n",
      "Validation Epoch 8/10 | Loss: 2.5794: 100%|██████████| 50/50 [00:01<00:00, 38.90it/s]\n",
      "Epoch 9/10 | Loss: 1.8766: 100%|██████████| 423/423 [00:11<00:00, 35.54it/s]\n",
      "Validation Epoch 9/10 | Loss: 2.5654: 100%|██████████| 50/50 [00:01<00:00, 38.78it/s]\n",
      "Epoch 10/10 | Loss: 1.8192: 100%|██████████| 423/423 [00:11<00:00, 35.97it/s]\n",
      "Validation Epoch 10/10 | Loss: 2.5076: 100%|██████████| 50/50 [00:01<00:00, 41.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final validation loss of outer fold: 2.5075659185647963\n",
      "Final best setting of outer fold: {'batch_size': 32, 'num_epochs': 10, 'device': 'cuda', 'optim_fn': <class 'torch.optim.adam.Adam'>, 'scheduler_fn': <class 'torch.optim.lr_scheduler.StepLR'>, 'scheduler_kwargs': {'step_size': 2, 'gamma': 0.8}, 'val_fraction': 0.1, 'val_split_by_year': True, 'optim_kwargs': {'lr': 0.001, 'weight_decay': 0.0001}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Loss: 32.7833: 100%|██████████| 422/422 [00:12<00:00, 32.88it/s]\n",
      "Validation Epoch 1/50 | Loss: 8.2974: 100%|██████████| 52/52 [00:01<00:00, 37.86it/s]\n",
      "Epoch 2/50 | Loss: 6.6138: 100%|██████████| 422/422 [00:12<00:00, 34.53it/s]\n",
      "Validation Epoch 2/50 | Loss: 4.9733: 100%|██████████| 52/52 [00:01<00:00, 38.92it/s]\n",
      "Epoch 3/50 | Loss: 6.0384: 100%|██████████| 422/422 [00:12<00:00, 34.59it/s]\n",
      "Validation Epoch 3/50 | Loss: 4.9098: 100%|██████████| 52/52 [00:01<00:00, 38.85it/s]\n",
      "Epoch 4/50 | Loss: 6.0352: 100%|██████████| 422/422 [00:12<00:00, 33.16it/s]\n",
      "Validation Epoch 4/50 | Loss: 4.8700: 100%|██████████| 52/52 [00:01<00:00, 40.27it/s]\n",
      "Epoch 5/50 | Loss: 6.0361: 100%|██████████| 422/422 [00:12<00:00, 33.78it/s]\n",
      "Validation Epoch 5/50 | Loss: 4.8904: 100%|██████████| 52/52 [00:01<00:00, 36.28it/s]\n",
      "Epoch 6/50 | Loss: 6.0347: 100%|██████████| 422/422 [00:12<00:00, 32.87it/s]\n",
      "Validation Epoch 6/50 | Loss: 4.8608: 100%|██████████| 52/52 [00:01<00:00, 38.55it/s]\n",
      "Epoch 7/50 | Loss: 6.0357: 100%|██████████| 422/422 [00:12<00:00, 34.15it/s]\n",
      "Validation Epoch 7/50 | Loss: 4.8607: 100%|██████████| 52/52 [00:01<00:00, 39.98it/s]\n",
      "Epoch 8/50 | Loss: 5.9223: 100%|██████████| 422/422 [00:12<00:00, 33.15it/s]\n",
      "Validation Epoch 8/50 | Loss: 4.1666: 100%|██████████| 52/52 [00:01<00:00, 38.54it/s]\n",
      "Epoch 9/50 | Loss: 3.8505: 100%|██████████| 422/422 [00:12<00:00, 34.15it/s]\n",
      "Validation Epoch 9/50 | Loss: 2.7205: 100%|██████████| 52/52 [00:01<00:00, 39.24it/s]\n",
      "Epoch 10/50 | Loss: 2.6154: 100%|██████████| 422/422 [00:12<00:00, 33.41it/s]\n",
      "Validation Epoch 10/50 | Loss: 2.6625: 100%|██████████| 52/52 [00:01<00:00, 38.72it/s]\n",
      "Epoch 11/50 | Loss: 2.2786: 100%|██████████| 422/422 [00:12<00:00, 33.40it/s]\n",
      "Validation Epoch 11/50 | Loss: 2.3095: 100%|██████████| 52/52 [00:01<00:00, 39.32it/s]\n",
      "Epoch 12/50 | Loss: 2.0709: 100%|██████████| 422/422 [00:12<00:00, 33.10it/s]\n",
      "Validation Epoch 12/50 | Loss: 2.3999: 100%|██████████| 52/52 [00:01<00:00, 38.06it/s]\n",
      "Epoch 13/50 | Loss: 1.9588: 100%|██████████| 422/422 [00:13<00:00, 32.39it/s]\n",
      "Validation Epoch 13/50 | Loss: 2.2333: 100%|██████████| 52/52 [00:01<00:00, 39.31it/s]\n",
      "Epoch 14/50 | Loss: 1.8999: 100%|██████████| 422/422 [00:12<00:00, 33.73it/s]\n",
      "Validation Epoch 14/50 | Loss: 2.1580: 100%|██████████| 52/52 [00:01<00:00, 37.61it/s]\n",
      "Epoch 15/50 | Loss: 1.8506: 100%|██████████| 422/422 [00:12<00:00, 33.84it/s]\n",
      "Validation Epoch 15/50 | Loss: 2.1525: 100%|██████████| 52/52 [00:01<00:00, 39.13it/s]\n",
      "Epoch 16/50 | Loss: 1.8097: 100%|██████████| 422/422 [00:12<00:00, 33.99it/s]\n",
      "Validation Epoch 16/50 | Loss: 2.2033: 100%|██████████| 52/52 [00:01<00:00, 36.71it/s]\n",
      "Epoch 17/50 | Loss: 1.7752: 100%|██████████| 422/422 [00:12<00:00, 32.51it/s]\n",
      "Validation Epoch 17/50 | Loss: 2.3523: 100%|██████████| 52/52 [00:01<00:00, 36.39it/s]\n",
      "Epoch 18/50 | Loss: 1.7396: 100%|██████████| 422/422 [00:12<00:00, 32.99it/s]\n",
      "Validation Epoch 18/50 | Loss: 2.2671: 100%|██████████| 52/52 [00:01<00:00, 36.29it/s]\n",
      "Epoch 19/50 | Loss: 1.7116: 100%|██████████| 422/422 [00:12<00:00, 33.05it/s]\n",
      "Validation Epoch 19/50 | Loss: 2.2467: 100%|██████████| 52/52 [00:01<00:00, 38.62it/s]\n",
      "Epoch 20/50 | Loss: 1.6620: 100%|██████████| 422/422 [00:12<00:00, 33.72it/s]\n",
      "Validation Epoch 20/50 | Loss: 2.3711: 100%|██████████| 52/52 [00:01<00:00, 39.93it/s]\n",
      "Epoch 21/50 | Loss: 1.6505: 100%|██████████| 422/422 [00:12<00:00, 32.99it/s]\n",
      "Validation Epoch 21/50 | Loss: 2.2341: 100%|██████████| 52/52 [00:01<00:00, 38.52it/s]\n",
      "Epoch 22/50 | Loss: 1.6324: 100%|██████████| 422/422 [00:12<00:00, 33.82it/s]\n",
      "Validation Epoch 22/50 | Loss: 2.2209: 100%|██████████| 52/52 [00:01<00:00, 38.51it/s]\n",
      "Epoch 23/50 | Loss: 1.6120: 100%|██████████| 422/422 [00:12<00:00, 33.45it/s]\n",
      "Validation Epoch 23/50 | Loss: 2.1455: 100%|██████████| 52/52 [00:01<00:00, 39.26it/s]\n",
      "Epoch 24/50 | Loss: 1.5733: 100%|██████████| 422/422 [00:12<00:00, 33.58it/s]\n",
      "Validation Epoch 24/50 | Loss: 2.3472: 100%|██████████| 52/52 [00:01<00:00, 39.45it/s]\n",
      "Epoch 25/50 | Loss: 1.5598: 100%|██████████| 422/422 [00:12<00:00, 32.75it/s]\n",
      "Validation Epoch 25/50 | Loss: 2.2396: 100%|██████████| 52/52 [00:01<00:00, 39.22it/s]\n",
      "Epoch 26/50 | Loss: 1.5442: 100%|██████████| 422/422 [00:12<00:00, 32.94it/s]\n",
      "Validation Epoch 26/50 | Loss: 2.2784: 100%|██████████| 52/52 [00:01<00:00, 39.02it/s]\n",
      "Epoch 27/50 | Loss: 1.5431: 100%|██████████| 422/422 [00:12<00:00, 33.26it/s]\n",
      "Validation Epoch 27/50 | Loss: 2.2750: 100%|██████████| 52/52 [00:01<00:00, 38.22it/s]\n",
      "Epoch 28/50 | Loss: 1.5088: 100%|██████████| 422/422 [00:12<00:00, 33.11it/s]\n",
      "Validation Epoch 28/50 | Loss: 2.3373: 100%|██████████| 52/52 [00:01<00:00, 37.87it/s]\n",
      "Epoch 29/50 | Loss: 1.4863: 100%|██████████| 422/422 [00:12<00:00, 32.99it/s]\n",
      "Validation Epoch 29/50 | Loss: 2.4173: 100%|██████████| 52/52 [00:01<00:00, 38.44it/s]\n",
      "Epoch 30/50 | Loss: 1.4835: 100%|██████████| 422/422 [00:13<00:00, 32.21it/s]\n",
      "Validation Epoch 30/50 | Loss: 2.3568: 100%|██████████| 52/52 [00:01<00:00, 37.67it/s]\n",
      "Epoch 31/50 | Loss: 1.4704: 100%|██████████| 422/422 [00:12<00:00, 33.55it/s]\n",
      "Validation Epoch 31/50 | Loss: 2.3046: 100%|██████████| 52/52 [00:01<00:00, 38.60it/s]\n",
      "Epoch 32/50 | Loss: 1.4621: 100%|██████████| 422/422 [00:12<00:00, 33.13it/s]\n",
      "Validation Epoch 32/50 | Loss: 2.5137: 100%|██████████| 52/52 [00:01<00:00, 36.28it/s]\n",
      "Epoch 33/50 | Loss: 1.4577: 100%|██████████| 422/422 [00:12<00:00, 33.57it/s]\n",
      "Validation Epoch 33/50 | Loss: 2.1596: 100%|██████████| 52/52 [00:01<00:00, 38.93it/s]\n",
      "Epoch 34/50 | Loss: 1.4366: 100%|██████████| 422/422 [00:12<00:00, 34.16it/s]\n",
      "Validation Epoch 34/50 | Loss: 2.2791: 100%|██████████| 52/52 [00:01<00:00, 40.75it/s]\n",
      "Epoch 35/50 | Loss: 1.4279: 100%|██████████| 422/422 [00:12<00:00, 32.87it/s]\n",
      "Validation Epoch 35/50 | Loss: 2.1943:  27%|██▋       | 14/52 [00:00<00:01, 35.31it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run the full benchmark for the best settings\u001b[39;00m\n\u001b[0;32m      2\u001b[0m run_kwargs \u001b[38;5;241m=\u001b[39m update_settings(settings[best_result_idx], base_args)\n\u001b[1;32m----> 3\u001b[0m result_df \u001b[38;5;241m=\u001b[39m \u001b[43mrun_benchmark\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_name_base\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_best\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name_base\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_best\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_constructor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(result_df)\n",
      "File \u001b[1;32mc:\\Users\\potze002\\Projects\\AgML-crop-yield-forecasting\\runs\\run_benchmark.py:131\u001b[0m, in \u001b[0;36mrun_benchmark\u001b[1;34m(run_name, model_name, model_constructor, model_init_kwargs, model_fit_kwargs, baseline_models, dataset_name)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model_constructor \u001b[38;5;129;01min\u001b[39;00m model_constructors\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    130\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_constructor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodels_init_kwargs[model_name])\n\u001b[1;32m--> 131\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(train_dataset, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodels_fit_kwargs[model_name])\n\u001b[0;32m    132\u001b[0m     predictions, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_dataset)\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# save predictions\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\potze002\\Projects\\AgML-crop-yield-forecasting\\models\\nn_models.py:94\u001b[0m, in \u001b[0;36mBaseNNModel.fit\u001b[1;34m(self, dataset, optimize_hyperparameters, param_space, do_kfold, kfolds, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_model, final_output\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 94\u001b[0m     model, output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_model(dataset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, output\n",
      "File \u001b[1;32mc:\\Users\\potze002\\Projects\\AgML-crop-yield-forecasting\\models\\nn_models.py:269\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(self, train_dataset, val_dataset, val_fraction, val_split_by_year, val_every_n_epochs, num_epochs, batch_size, loss_fn, loss_kwargs, optim_fn, optim_kwargs, scheduler_fn, scheduler_kwargs, device, **fit_params)\u001b[0m\n\u001b[0;32m    266\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(predictions, target, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    270\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    271\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\potze002\\AppData\\Local\\mambaforge\\envs\\gxemaize\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\potze002\\AppData\\Local\\mambaforge\\envs\\gxemaize\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\potze002\\AppData\\Local\\mambaforge\\envs\\gxemaize\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\potze002\\AppData\\Local\\mambaforge\\envs\\gxemaize\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\potze002\\AppData\\Local\\mambaforge\\envs\\gxemaize\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\potze002\\Projects\\AgML-crop-yield-forecasting\\datasets\\dataset_torch.py:25\u001b[0m, in \u001b[0;36mTorchDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    Get a sample from the dataset\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m    Data is cast to PyTorch tensors where required\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m    :param index: index that is passed to the dataset\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    :return: a dict containing the data sample\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cast_to_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\potze002\\Projects\\AgML-crop-yield-forecasting\\datasets\\dataset_torch.py:44\u001b[0m, in \u001b[0;36mTorchDataset._cast_to_tensor\u001b[1;34m(cls, sample)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cast_to_tensor\u001b[39m(\u001b[38;5;28mcls\u001b[39m, sample: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    Create a sample with all data cast to torch tensors\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m    :param sample: the sample to convert\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    :return: the converted data sample\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     40\u001b[0m         KEY_LOC: sample[KEY_LOC],\n\u001b[0;32m     41\u001b[0m         KEY_YEAR: sample[KEY_YEAR],\n\u001b[0;32m     42\u001b[0m         KEY_TARGET: torch\u001b[38;5;241m.\u001b[39mtensor(sample[KEY_TARGET], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m     43\u001b[0m         KEY_DATES: sample[KEY_DATES],\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\n\u001b[0;32m     45\u001b[0m             key: torch\u001b[38;5;241m.\u001b[39mtensor(sample[key], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     46\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m sample\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m     47\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [KEY_LOC, KEY_YEAR, KEY_TARGET, KEY_DATES]\n\u001b[0;32m     48\u001b[0m         },  \u001b[38;5;66;03m# TODO -- support nonnumeric data?\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     }\n",
      "File \u001b[1;32mc:\\Users\\potze002\\Projects\\AgML-crop-yield-forecasting\\datasets\\dataset_torch.py:45\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cast_to_tensor\u001b[39m(\u001b[38;5;28mcls\u001b[39m, sample: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    Create a sample with all data cast to torch tensors\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m    :param sample: the sample to convert\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    :return: the converted data sample\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     40\u001b[0m         KEY_LOC: sample[KEY_LOC],\n\u001b[0;32m     41\u001b[0m         KEY_YEAR: sample[KEY_YEAR],\n\u001b[0;32m     42\u001b[0m         KEY_TARGET: torch\u001b[38;5;241m.\u001b[39mtensor(sample[KEY_TARGET], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m     43\u001b[0m         KEY_DATES: sample[KEY_DATES],\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\n\u001b[1;32m---> 45\u001b[0m             key: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m sample\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m     47\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [KEY_LOC, KEY_YEAR, KEY_TARGET, KEY_DATES]\n\u001b[0;32m     48\u001b[0m         },  \u001b[38;5;66;03m# TODO -- support nonnumeric data?\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     }\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load best settings dict from file with pickle\n",
    "import pickle\n",
    "with open(f\"results/{run_name_base}/best_settings_dict.pkl\", \"rb\") as f:\n",
    "    best_settings = pickle.load(f)\n",
    "\n",
    "\n",
    "# Assert that the best settings are the same as the best settings found if settings exist\n",
    "if best_result_idx is not None: \n",
    "    assert best_settings == settings[best_result_idx]\n",
    "\n",
    "\n",
    "# Run the full benchmark for the best settings\n",
    "run_kwargs = update_settings(best_settings, base_args)\n",
    "result_df = run_benchmark(\n",
    "    run_name_base + '_best',\n",
    "    model_name_base + '_best',\n",
    "    model_constructor,\n",
    "    run_kwargs['init'],\n",
    "    run_kwargs['fit'],\n",
    ")\n",
    "\n",
    "print(result_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the evaluation results\n",
    "from runs.run_benchmark import _compute_evaluation_results\n",
    "_compute_evaluation_results(run_name_base + '_best')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gxemaize",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
